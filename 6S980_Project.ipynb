{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3_IvSlv8esf"
   },
   "source": [
    "## Directory Structure\n",
    "PLEASE make sure your directory looks like this:\n",
    "- root_dir\n",
    "    - pixel-nerf (clone/pull from github)\n",
    "    - adaptive-volume-rendering (clone/pull from github)\n",
    "        - 6S980_Project.ipynb\n",
    "    - scene-representation-networks (clone/pull from github)\n",
    "    - data\n",
    "    - checkpoints\n",
    "    - ......\n",
    "    \n",
    "Then set your pwd to root_dir\n",
    "\n",
    "## Relevant Repos\n",
    "\n",
    "- PixelNeRF repo: https://github.com/sxyu/pixel-nerf\n",
    "\n",
    "- SRN repo: https://github.com/vsitzmann/scene-representation-networks\n",
    "\n",
    "- SRN raymarcher algorithm: https://github.com/vsitzmann/scene-representation-networks/blob/master/custom_layers.py\n",
    "\n",
    "## Data\n",
    "\n",
    "- Currently using cars dataset from SRN\n",
    "\n",
    "## What we did\n",
    "\n",
    "- Now the pixelnerf with pretrained weights are running!\n",
    "\n",
    "\n",
    "## Questions\n",
    "- PixelNeRF currently does: evenly coarse sampling - fine sampling via importance weights - fine sampling around expected depth. With adaptive procedure, what is a fair comparison?\n",
    "\n",
    "## TODOs\n",
    "- First attempt: let LSTM interact with the features, do 10 steps, then directly output a color\n",
    "- Second attempt: sample points around the final location and do volume integral\n",
    "\n",
    "- Use view 64 to render out videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yankesong/Documents/YankeSong/Harvard Textbook/MIT 6.S980/Project\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed Torch version: 1.13.0\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"/Users/yankesong/Documents/YankeSong/Harvard Textbook/MIT 6.S980/Project/\"       # This is the only thing you need to change.\n",
    "%cd \"/Users/yankesong/Documents/YankeSong/Harvard Textbook/MIT 6.S980/Project/\"\n",
    "\n",
    "# root_dir = \"/Users/jameszli/desktop/MIT/6.S980/\"       # This is the only thing you need to change.\n",
    "# %cd \"/Users/jameszli/desktop/MIT/6.S980/\"\n",
    "\n",
    "# root_dir = \"/home/ysong/project/\"       # This is the only thing you need to change.\n",
    "# %cd \"/home/ysong/project/\" \n",
    "\n",
    "# # Install everything\n",
    "# %pip install -r adaptive-volume-rendering/requirements.txt\n",
    "\n",
    "# Import everything\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, f\"{root_dir}/pixel-nerf/src/\")\n",
    "sys.path.insert(0, f\"{root_dir}/adaptive-volume-rendering/\")\n",
    "\n",
    "from model import make_model, loss\n",
    "from dataset import *\n",
    "from models import *\n",
    "from renderers import *\n",
    "from trains import *\n",
    "from utils import *\n",
    "\n",
    "\n",
    "print(f\"Installed Torch version: {torch.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9h59cKEDM6O"
   },
   "source": [
    "## Setup & Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "id": "wcct4u0iHywz",
    "outputId": "124f87f7-c70a-4612-9d70-87a1af0cb7cd"
   },
   "outputs": [],
   "source": [
    "# # Download data and weights\n",
    "\n",
    "# # Make a new directory\n",
    "# !mkdir data\n",
    "# !mkdir checkpoints\n",
    "\n",
    "# # Download Test Dataset\n",
    "# if not os.path.exists(f\"{root_dir}data/cars_train.hdf5\"):\n",
    "#     # Download SRNs-cars dataset\n",
    "#     gdown.download(\"https://drive.google.com/uc?id={}\".format(\"1SBjlsizq0sFNkCZxMQh-pNRi0HyFozKb\"),f\"{root_dir}data/cars_train.hdf5\")\n",
    "\n",
    "\n",
    "# ## Download pretrained weights from PixelNeRF\n",
    "# if not os.path.exists(f\"{root_dir}checkpoints/pixel_nerf_weights.zip\"):\n",
    "#   gdown.download(\"https://drive.google.com/uc?id={}\".format(\"1UO_rL201guN6euoWkCOn-XpqR2e8o6ju\"),f\"{root_dir}checkpoints/pixel_nerf_weights.zip\")\n",
    "#   !unzip checkpoints/pixel_nerf_weights.zip -d checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybadPLmoE4a4"
   },
   "source": [
    "## Volume Renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "id": "ExobcYT0XFpg",
    "outputId": "d814ab7f-b8ca-4cad-93e7-54b794fb9466"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torchvision resnet34 encoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.model.conv1.weight\n",
      "encoder.model.bn1.weight\n",
      "encoder.model.bn1.bias\n",
      "encoder.model.layer1.0.conv1.weight\n",
      "encoder.model.layer1.0.bn1.weight\n",
      "encoder.model.layer1.0.bn1.bias\n",
      "encoder.model.layer1.0.conv2.weight\n",
      "encoder.model.layer1.0.bn2.weight\n",
      "encoder.model.layer1.0.bn2.bias\n",
      "encoder.model.layer1.1.conv1.weight\n",
      "encoder.model.layer1.1.bn1.weight\n",
      "encoder.model.layer1.1.bn1.bias\n",
      "encoder.model.layer1.1.conv2.weight\n",
      "encoder.model.layer1.1.bn2.weight\n",
      "encoder.model.layer1.1.bn2.bias\n",
      "encoder.model.layer1.2.conv1.weight\n",
      "encoder.model.layer1.2.bn1.weight\n",
      "encoder.model.layer1.2.bn1.bias\n",
      "encoder.model.layer1.2.conv2.weight\n",
      "encoder.model.layer1.2.bn2.weight\n",
      "encoder.model.layer1.2.bn2.bias\n",
      "encoder.model.layer2.0.conv1.weight\n",
      "encoder.model.layer2.0.bn1.weight\n",
      "encoder.model.layer2.0.bn1.bias\n",
      "encoder.model.layer2.0.conv2.weight\n",
      "encoder.model.layer2.0.bn2.weight\n",
      "encoder.model.layer2.0.bn2.bias\n",
      "encoder.model.layer2.0.downsample.0.weight\n",
      "encoder.model.layer2.0.downsample.1.weight\n",
      "encoder.model.layer2.0.downsample.1.bias\n",
      "encoder.model.layer2.1.conv1.weight\n",
      "encoder.model.layer2.1.bn1.weight\n",
      "encoder.model.layer2.1.bn1.bias\n",
      "encoder.model.layer2.1.conv2.weight\n",
      "encoder.model.layer2.1.bn2.weight\n",
      "encoder.model.layer2.1.bn2.bias\n",
      "encoder.model.layer2.2.conv1.weight\n",
      "encoder.model.layer2.2.bn1.weight\n",
      "encoder.model.layer2.2.bn1.bias\n",
      "encoder.model.layer2.2.conv2.weight\n",
      "encoder.model.layer2.2.bn2.weight\n",
      "encoder.model.layer2.2.bn2.bias\n",
      "encoder.model.layer2.3.conv1.weight\n",
      "encoder.model.layer2.3.bn1.weight\n",
      "encoder.model.layer2.3.bn1.bias\n",
      "encoder.model.layer2.3.conv2.weight\n",
      "encoder.model.layer2.3.bn2.weight\n",
      "encoder.model.layer2.3.bn2.bias\n",
      "encoder.model.layer3.0.conv1.weight\n",
      "encoder.model.layer3.0.bn1.weight\n",
      "encoder.model.layer3.0.bn1.bias\n",
      "encoder.model.layer3.0.conv2.weight\n",
      "encoder.model.layer3.0.bn2.weight\n",
      "encoder.model.layer3.0.bn2.bias\n",
      "encoder.model.layer3.0.downsample.0.weight\n",
      "encoder.model.layer3.0.downsample.1.weight\n",
      "encoder.model.layer3.0.downsample.1.bias\n",
      "encoder.model.layer3.1.conv1.weight\n",
      "encoder.model.layer3.1.bn1.weight\n",
      "encoder.model.layer3.1.bn1.bias\n",
      "encoder.model.layer3.1.conv2.weight\n",
      "encoder.model.layer3.1.bn2.weight\n",
      "encoder.model.layer3.1.bn2.bias\n",
      "encoder.model.layer3.2.conv1.weight\n",
      "encoder.model.layer3.2.bn1.weight\n",
      "encoder.model.layer3.2.bn1.bias\n",
      "encoder.model.layer3.2.conv2.weight\n",
      "encoder.model.layer3.2.bn2.weight\n",
      "encoder.model.layer3.2.bn2.bias\n",
      "encoder.model.layer3.3.conv1.weight\n",
      "encoder.model.layer3.3.bn1.weight\n",
      "encoder.model.layer3.3.bn1.bias\n",
      "encoder.model.layer3.3.conv2.weight\n",
      "encoder.model.layer3.3.bn2.weight\n",
      "encoder.model.layer3.3.bn2.bias\n",
      "encoder.model.layer3.4.conv1.weight\n",
      "encoder.model.layer3.4.bn1.weight\n",
      "encoder.model.layer3.4.bn1.bias\n",
      "encoder.model.layer3.4.conv2.weight\n",
      "encoder.model.layer3.4.bn2.weight\n",
      "encoder.model.layer3.4.bn2.bias\n",
      "encoder.model.layer3.5.conv1.weight\n",
      "encoder.model.layer3.5.bn1.weight\n",
      "encoder.model.layer3.5.bn1.bias\n",
      "encoder.model.layer3.5.conv2.weight\n",
      "encoder.model.layer3.5.bn2.weight\n",
      "encoder.model.layer3.5.bn2.bias\n",
      "encoder.model.layer4.0.conv1.weight\n",
      "encoder.model.layer4.0.bn1.weight\n",
      "encoder.model.layer4.0.bn1.bias\n",
      "encoder.model.layer4.0.conv2.weight\n",
      "encoder.model.layer4.0.bn2.weight\n",
      "encoder.model.layer4.0.bn2.bias\n",
      "encoder.model.layer4.0.downsample.0.weight\n",
      "encoder.model.layer4.0.downsample.1.weight\n",
      "encoder.model.layer4.0.downsample.1.bias\n",
      "encoder.model.layer4.1.conv1.weight\n",
      "encoder.model.layer4.1.bn1.weight\n",
      "encoder.model.layer4.1.bn1.bias\n",
      "encoder.model.layer4.1.conv2.weight\n",
      "encoder.model.layer4.1.bn2.weight\n",
      "encoder.model.layer4.1.bn2.bias\n",
      "encoder.model.layer4.2.conv1.weight\n",
      "encoder.model.layer4.2.bn1.weight\n",
      "encoder.model.layer4.2.bn1.bias\n",
      "encoder.model.layer4.2.conv2.weight\n",
      "encoder.model.layer4.2.bn2.weight\n",
      "encoder.model.layer4.2.bn2.bias\n",
      "mlp_coarse.lin_in.weight\n",
      "mlp_coarse.lin_in.bias\n",
      "mlp_coarse.lin_out.weight\n",
      "mlp_coarse.lin_out.bias\n",
      "mlp_coarse.blocks.0.fc_0.weight\n",
      "mlp_coarse.blocks.0.fc_0.bias\n",
      "mlp_coarse.blocks.0.fc_1.weight\n",
      "mlp_coarse.blocks.0.fc_1.bias\n",
      "mlp_coarse.blocks.1.fc_0.weight\n",
      "mlp_coarse.blocks.1.fc_0.bias\n",
      "mlp_coarse.blocks.1.fc_1.weight\n",
      "mlp_coarse.blocks.1.fc_1.bias\n",
      "mlp_coarse.blocks.2.fc_0.weight\n",
      "mlp_coarse.blocks.2.fc_0.bias\n",
      "mlp_coarse.blocks.2.fc_1.weight\n",
      "mlp_coarse.blocks.2.fc_1.bias\n",
      "mlp_coarse.blocks.3.fc_0.weight\n",
      "mlp_coarse.blocks.3.fc_0.bias\n",
      "mlp_coarse.blocks.3.fc_1.weight\n",
      "mlp_coarse.blocks.3.fc_1.bias\n",
      "mlp_coarse.blocks.4.fc_0.weight\n",
      "mlp_coarse.blocks.4.fc_0.bias\n",
      "mlp_coarse.blocks.4.fc_1.weight\n",
      "mlp_coarse.blocks.4.fc_1.bias\n",
      "mlp_coarse.lin_z.0.weight\n",
      "mlp_coarse.lin_z.0.bias\n",
      "mlp_coarse.lin_z.1.weight\n",
      "mlp_coarse.lin_z.1.bias\n",
      "mlp_coarse.lin_z.2.weight\n",
      "mlp_coarse.lin_z.2.bias\n",
      "mlp_fine.lin_in.weight\n",
      "mlp_fine.lin_in.bias\n",
      "mlp_fine.lin_out.weight\n",
      "mlp_fine.lin_out.bias\n",
      "mlp_fine.blocks.0.fc_0.weight\n",
      "mlp_fine.blocks.0.fc_0.bias\n",
      "mlp_fine.blocks.0.fc_1.weight\n",
      "mlp_fine.blocks.0.fc_1.bias\n",
      "mlp_fine.blocks.1.fc_0.weight\n",
      "mlp_fine.blocks.1.fc_0.bias\n",
      "mlp_fine.blocks.1.fc_1.weight\n",
      "mlp_fine.blocks.1.fc_1.bias\n",
      "mlp_fine.blocks.2.fc_0.weight\n",
      "mlp_fine.blocks.2.fc_0.bias\n",
      "mlp_fine.blocks.2.fc_1.weight\n",
      "mlp_fine.blocks.2.fc_1.bias\n",
      "mlp_fine.blocks.3.fc_0.weight\n",
      "mlp_fine.blocks.3.fc_0.bias\n",
      "mlp_fine.blocks.3.fc_1.weight\n",
      "mlp_fine.blocks.3.fc_1.bias\n",
      "mlp_fine.blocks.4.fc_0.weight\n",
      "mlp_fine.blocks.4.fc_0.bias\n",
      "mlp_fine.blocks.4.fc_1.weight\n",
      "mlp_fine.blocks.4.fc_1.bias\n",
      "mlp_fine.lin_z.0.weight\n",
      "mlp_fine.lin_z.0.bias\n",
      "mlp_fine.lin_z.1.weight\n",
      "mlp_fine.lin_z.1.bias\n",
      "mlp_fine.lin_z.2.weight\n",
      "mlp_fine.lin_z.2.bias\n"
     ]
    }
   ],
   "source": [
    "# ############## Original Volume renderer\n",
    "# Create a custom conf\n",
    "# from pyhocon import ConfigFactory\n",
    "# conf = ConfigFactory.parse_file(f\"adaptive-volume-rendering/conf/default_mv.conf\")\n",
    "\n",
    "# # Create a pixelnerf net\n",
    "# net = make_new_model(conf[\"model\"]).to(device=device)\n",
    "# net.stop_encoder_grad = True\n",
    "\n",
    "# # Load pretrianed weights\n",
    "# # model_path = f\"{root_dir}checkpoints/srn_car/pixel_nerf_latest\"\n",
    "# # net.load_weights(model_path)\n",
    "\n",
    "# # Combine with volumerenderer\n",
    "# renderer = VolumeRenderer.from_conf(conf[\"normal_renderer\"]).to(\n",
    "#     device=device\n",
    "# )\n",
    "# max_num_instances = 2150\n",
    "# samples_per_instance = 8\n",
    "\n",
    "# rf_and_renderer = RadFieldAndRenderer(net, renderer).to(device=device)\n",
    "# model_path = f\"{root_dir}checkpoints/experiment/vr_{max_num_instances}cars_{samples_per_instance}samples_40000steps\"\n",
    "# rf_and_renderer.load_weights(model_path)\n",
    "# # Test dataset\n",
    "# sl = 32\n",
    "# max_num_instances = 2150\n",
    "# train_dataset = SceneClassDataset(root_dir=f\"{root_dir}data/cars_train\",\n",
    "#                                              max_num_instances=10,\n",
    "#                                              max_observations_per_instance=250,\n",
    "#                                              img_sidelength=sl,\n",
    "#                                              specific_observation_idcs=None,\n",
    "#                                              samples_per_instance=samples_per_instance)\n",
    "\n",
    "\n",
    "# train_dataloader = DataLoader(train_dataset,\n",
    "#                                       batch_size=1,\n",
    "#                                       shuffle=True,\n",
    "#                                       drop_last=True,\n",
    "#                                       collate_fn=train_dataset.collate_fn\n",
    "#                                       )\n",
    "\n",
    "# # Configurations\n",
    "# optim = torch.optim.Adam(lr=1e-4, params=rf_and_renderer.parameters())\n",
    "\n",
    "    \n",
    "# try:\n",
    "#     _ = fit(net,\n",
    "#         rf_and_renderer, \n",
    "#         train_dataloader, \n",
    "#         loss_fn=mse_loss, \n",
    "#         resolution=(sl, sl, 3), \n",
    "#         plotting_function=plot_output_ground_truth, \n",
    "#         optimizer=optim, \n",
    "#         total_steps=100,\n",
    "#         steps_til_summary=1\n",
    "#         )\n",
    "#     model_path = f\"{root_dir}checkpoints/experiment/vr_{max_num_instances}cars_{samples_per_instance}samples_40000steps\"\n",
    "#     rf_and_renderer.save_weights(model_path)\n",
    "# except Exception:\n",
    "#     print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PixelNeRF + Raymarcher only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a custom conf\n",
    "# from pyhocon import ConfigFactory\n",
    "# conf = ConfigFactory.parse_file(f\"{root_dir}adaptive-volume-rendering/conf/default_mv.conf\")\n",
    "\n",
    "# # Create a pixelnerf net\n",
    "# net = make_new_model(conf[\"model\"]).to(device=device)\n",
    "# net.stop_encoder_grad = True\n",
    "# # Combine with volumerenderer\n",
    "# renderer = Raymarcher.from_conf(conf[\"raymarcher\"]).to(\n",
    "#     device=device\n",
    "# )\n",
    "\n",
    "# rf_and_renderer = RadFieldAndRenderer(net, renderer).to(device=device)\n",
    "# model_path = f\"{root_dir}checkpoints/experiment/newraymarcher_2150cars_8samples_50ksteps\"\n",
    "# rf_and_renderer.load_weights(model_path)\n",
    "# # own_state = rf_and_renderer.state_dict()\n",
    "# # load_state = torch.load(model_path, map_location=device)\n",
    "# # for name, param in load_state.items():\n",
    "# #     if name not in own_state:\n",
    "# #         continue\n",
    "# #     if isinstance(param, nn.Parameter):\n",
    "# #         # backwards compatibility for serialized parameters\n",
    "# #         param = param.data\n",
    "# #     own_state[name].copy_(param)\n",
    "\n",
    "# max_num_instances = 2150\n",
    "# samples_per_instance = 8\n",
    "\n",
    "# # Freeze nerf weights\n",
    "# for param in net.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "\n",
    "# ######## TODO\n",
    "# # Test dataset\n",
    "# sl = 32\n",
    "# max_num_instances = 2150\n",
    "# train_dataset = SceneClassDataset(root_dir=f\"{root_dir}data/cars_train\",\n",
    "#                                              max_num_instances=max_num_instances,\n",
    "#                                              max_observations_per_instance=250,\n",
    "#                                              img_sidelength=sl,\n",
    "#                                              specific_observation_idcs=None,\n",
    "#                                              samples_per_instance=samples_per_instance)\n",
    "\n",
    "\n",
    "# train_dataloader = DataLoader(train_dataset,\n",
    "#                                       batch_size=1,\n",
    "#                                       shuffle=True,\n",
    "#                                       drop_last=True,\n",
    "#                                       collate_fn=train_dataset.collate_fn\n",
    "#                                       )\n",
    "\n",
    "# # Configurations\n",
    "# optim = torch.optim.Adam(lr=1e-4, params=rf_and_renderer.parameters())\n",
    "\n",
    "# try:\n",
    "#     _ = fit(net,\n",
    "#         rf_and_renderer, \n",
    "#         train_dataloader, \n",
    "#         loss_fn=mse_regularization_loss, \n",
    "#         resolution=(sl, sl, 3), \n",
    "#         plotting_function=plot_output_ground_truth, \n",
    "#         optimizer=optim, \n",
    "#         total_steps=50000,\n",
    "#         steps_til_summary=200\n",
    "#         )\n",
    "#     model_path = f\"{root_dir}checkpoints/experiment/newraymarcher_{max_num_instances}cars_{samples_per_instance}samples_100ksteps\"\n",
    "#     rf_and_renderer.save_weights(model_path)\n",
    "# except Exception:\n",
    "#     print(traceback.format_exc())\n",
    "    \n",
    "# try:\n",
    "#     _ = fit(net,\n",
    "#         rf_and_renderer, \n",
    "#         train_dataloader, \n",
    "#         loss_fn=mse_regularization_loss, \n",
    "#         resolution=(sl, sl, 3), \n",
    "#         plotting_function=plot_output_ground_truth, \n",
    "#         optimizer=optim, \n",
    "#         total_steps=50000,\n",
    "#         steps_til_summary=200\n",
    "#         )\n",
    "#     model_path = f\"{root_dir}checkpoints/experiment/newraymarcher_{max_num_instances}cars_{samples_per_instance}samples_150ksteps\"\n",
    "#     rf_and_renderer.save_weights(model_path)\n",
    "# except Exception:\n",
    "#     print(traceback.format_exc())\n",
    "    \n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PixelNeRF + Raymarcher, retrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a custom conf\n",
    "# from pyhocon import ConfigFactory\n",
    "# conf = ConfigFactory.parse_file(f\"{root_dir}adaptive-volume-rendering/conf/default_mv.conf\")\n",
    "\n",
    "# # Create a pixelnerf net\n",
    "# net = make_new_model(conf[\"model\"]).to(device=device)\n",
    "# net.stop_encoder_grad = True\n",
    "# # model_path = f\"{root_dir}checkpoints/srn_car/pixel_nerf_latest\"\n",
    "# # net.load_weights(model_path)\n",
    "# # Combine with volumerenderer\n",
    "# renderer = Raymarcher.from_conf(conf[\"raymarcher\"]).to(\n",
    "#     device=device\n",
    "# )\n",
    "\n",
    "# rf_and_renderer = RadFieldAndRenderer(net, renderer).to(device=device)\n",
    "\n",
    "# max_num_instances = 2150\n",
    "# samples_per_instance = 8\n",
    "\n",
    "# # Load pretrianed weights\n",
    "\n",
    "# model_path = f\"{root_dir}checkpoints/experiment/newraymarcher_{max_num_instances}cars_{samples_per_instance}samples_400ksteps\"\n",
    "# rf_and_renderer.load_weights(model_path)\n",
    "\n",
    "\n",
    "# # Freeze nerf weights\n",
    "# # for param in net.parameters():\n",
    "# #     param.requires_grad = False\n",
    "\n",
    "\n",
    "# ######## TODO\n",
    "# # Test dataset\n",
    "# sl = 32\n",
    "# max_num_instances = 2150\n",
    "# train_dataset = SceneClassDataset(root_dir=f\"{root_dir}data/cars_train\",\n",
    "#                                              max_num_instances=max_num_instances,\n",
    "#                                              max_observations_per_instance=250,\n",
    "#                                              img_sidelength=sl,\n",
    "#                                              specific_observation_idcs=None,\n",
    "#                                              samples_per_instance=samples_per_instance)\n",
    "\n",
    "\n",
    "# train_dataloader = DataLoader(train_dataset,\n",
    "#                                       batch_size=1,\n",
    "#                                       shuffle=True,\n",
    "#                                       drop_last=True,\n",
    "#                                       collate_fn=train_dataset.collate_fn\n",
    "#                                       )\n",
    "\n",
    "# # Configurations\n",
    "# optim = torch.optim.Adam(lr=1e-4, params=rf_and_renderer.parameters())\n",
    "\n",
    "# try:\n",
    "#     _ = fit(net,\n",
    "#         rf_and_renderer, \n",
    "#         train_dataloader, \n",
    "#         loss_fn=mse_regularization_loss, \n",
    "#         resolution=(sl, sl, 3), \n",
    "#         plotting_function=plot_output_ground_truth, \n",
    "#         optimizer=optim, \n",
    "#         total_steps=50000,\n",
    "#         steps_til_summary=200\n",
    "#         )\n",
    "#     model_path = f\"{root_dir}checkpoints/experiment/newraymarcher_{max_num_instances}cars_{samples_per_instance}samples_500ksteps\"\n",
    "#     rf_and_renderer.save_weights(model_path)\n",
    "# except Exception:\n",
    "#     print(traceback.format_exc())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PixelNeRF + Adaptive Volume Renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a custom conf\n",
    "# from pyhocon import ConfigFactory\n",
    "# conf = ConfigFactory.parse_file(f\"adaptive-volume-rendering/conf/default_mv.conf\")\n",
    "\n",
    "# # Create a pixelnerf net\n",
    "# net = make_new_model(conf[\"model\"]).to(device=device)\n",
    "# net.stop_encoder_grad = True\n",
    "\n",
    "# # Combine with volumerenderer\n",
    "# renderer = AdaptiveVolumeRenderer.from_conf(conf[\"adaptive_renderer\"]).to(device=device)\n",
    "\n",
    "# rf_and_renderer = RadFieldAndRenderer(net, renderer).to(device=device)\n",
    "\n",
    "\n",
    "# max_num_instances = 2150\n",
    "# samples_per_instance = 8\n",
    "\n",
    "# # Load pretrianed weights\n",
    "# model_path = f\"{root_dir}checkpoints/experiment/avr_{max_num_instances}cars_{samples_per_instance}samples_20ksteps\"\n",
    "# rf_and_renderer.load_weights(model_path)\n",
    "\n",
    "# # own_state = rf_and_renderer.state_dict()\n",
    "# # load_state = torch.load(model_path, map_location=device)\n",
    "# # for name, param in load_state.items():\n",
    "# #     if name not in own_state:\n",
    "# #         continue\n",
    "# #     if isinstance(param, nn.Parameter):\n",
    "# #         # backwards compatibility for serialized parameters\n",
    "# #         param = param.data\n",
    "# #     own_state[name].copy_(param)\n",
    "    \n",
    "\n",
    "# ######## TODO\n",
    "# # Test dataset\n",
    "# sl = 32\n",
    "\n",
    "# train_dataset = SceneClassDataset(root_dir=f\"{root_dir}data/cars_train\",\n",
    "#                                              max_num_instances=max_num_instances,\n",
    "#                                              max_observations_per_instance=250,\n",
    "#                                              img_sidelength=sl,\n",
    "#                                              specific_observation_idcs=None,\n",
    "#                                              samples_per_instance=samples_per_instance)\n",
    "\n",
    "\n",
    "# train_dataloader = DataLoader(train_dataset,\n",
    "#                                       batch_size=1,\n",
    "#                                       shuffle=True,\n",
    "#                                       drop_last=True,\n",
    "#                                       collate_fn=train_dataset.collate_fn\n",
    "#                                       )\n",
    "\n",
    "# # Configurations\n",
    "# optim = torch.optim.Adam(lr=1e-4, params=rf_and_renderer.parameters())\n",
    "\n",
    "# try:\n",
    "#     _ = fit(net,\n",
    "#         rf_and_renderer, \n",
    "#         train_dataloader, \n",
    "#         loss_fn=mse_loss, \n",
    "#         resolution=(sl, sl, 3), \n",
    "#         plotting_function=plot_output_ground_truth, \n",
    "#         optimizer=optim, \n",
    "#         total_steps=20000,\n",
    "#         steps_til_summary=100\n",
    "#         )\n",
    "#     model_path = f\"{root_dir}checkpoints/experiment/avr_{max_num_instances}cars_{samples_per_instance}samples_40ksteps\"\n",
    "#     rf_and_renderer.save_weights(model_path)\n",
    "# except Exception:\n",
    "#     print(traceback.format_exc())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torchvision resnet34 encoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ysong/anaconda3/envs/adaptivevolumerendering/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ysong/anaconda3/envs/adaptivevolumerendering/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load /home/ysong/project/checkpoints/experiment/avr_2150cars_8samples_40ksteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ysong/anaconda3/envs/adaptivevolumerendering/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1666642991888/work/aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it takes 13.298009395599365 seconds to render a video\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=400 controls autoplay loop>\n",
       "      <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Volume Renderer\n",
    "\n",
    "max_num_instances = 2150\n",
    "samples_per_instance = 8\n",
    "\n",
    "# Import model\n",
    "from pyhocon import ConfigFactory\n",
    "conf = ConfigFactory.parse_file(f\"{root_dir}adaptive-volume-rendering/conf/default_mv.conf\")\n",
    "\n",
    "net = make_new_model(conf[\"model\"]).to(device=device)\n",
    "net.stop_encoder_grad = True\n",
    "\n",
    "# renderer = Raymarcher.from_conf(conf[\"raymarcher\"]).to(device=device)\n",
    "# renderer = VolumeRenderer.from_conf(conf[\"normal_renderer\"]).to(device=device)\n",
    "renderer = AdaptiveVolumeRenderer.from_conf(conf[\"adaptive_renderer\"]).to(device=device)\n",
    "rf_and_renderer = RadFieldAndRenderer(net, renderer).to(device=device)\n",
    "\n",
    "# Load pretrianed weights\n",
    "# model_path = f\"{root_dir}checkpoints/experiment/newraymarcher_{max_num_instances}cars_{samples_per_instance}samples_400ksteps\"\n",
    "# model_path = f\"{root_dir}checkpoints/experiment/vr_{max_num_instances}cars_{samples_per_instance}samples_40000steps\"\n",
    "model_path = f\"{root_dir}checkpoints/experiment/avr_{max_num_instances}cars_{samples_per_instance}samples_40ksteps\"\n",
    "\n",
    "rf_and_renderer.load_weights(model_path)\n",
    "\n",
    "\n",
    "# Load test image\n",
    "sl = 32\n",
    "video_dataset = SceneClassDataset(root_dir=f\"{root_dir}data/cars_val\",\n",
    "                                             max_num_instances=1,\n",
    "                                             max_observations_per_instance=250,\n",
    "                                             img_sidelength=sl,\n",
    "                                             specific_observation_idcs=[64],\n",
    "                                             samples_per_instance=1)\n",
    "\n",
    "\n",
    "video_dataloader = DataLoader(video_dataset,\n",
    "                                      batch_size=1,\n",
    "                                      shuffle=True,\n",
    "                                      drop_last=True,\n",
    "                                      collate_fn=video_dataset.collate_fn\n",
    "                                      )\n",
    "\n",
    "video_mi = next(iter(video_dataloader))\n",
    "plt.imshow(video_mi['images'][0].reshape(sl,sl,3))\n",
    "plt.savefig('videos/img.png')\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "frames = generate_video(video_mi, 360, 1.5, net, rf_and_renderer)\n",
    "end = time.time()\n",
    "print(f'it takes {end - start} seconds to render a video')\n",
    "\n",
    "# Make a video\n",
    "type = 'avr'\n",
    "f = f'videos/video_{type}.gif'\n",
    "imageio.mimwrite(f, frames, fps=60)\n",
    "f = f'videos/video_{type}.mp4'\n",
    "imageio.mimwrite(f, frames, fps=60, quality=7)\n",
    "\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "mp4 = open(f,'rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "HTML(\"\"\"\n",
    "<video width=400 controls autoplay loop>\n",
    "      <source src=\"%s\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\" % data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torchvision resnet34 encoder\n",
      "Load /home/ysong/project/checkpoints/experiment/avr_2150cars_8samples_40ksteps\n",
      "object number 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ysong/project//adaptive-volume-rendering/utils.py:359: UserWarning: DEPRECATED: skimage.measure.compare_ssim has been moved to skimage.metrics.structural_similarity. It will be removed from skimage.measure in version 0.18.\n",
      "  ssim = skimage.measure.compare_ssim(rgb, gt, multichannel=True, data_range=1)\n",
      "/home/ysong/project//adaptive-volume-rendering/utils.py:360: UserWarning: DEPRECATED: skimage.measure.compare_psnr has been moved to skimage.metrics.peak_signal_noise_ratio. It will be removed from skimage.measure in version 0.18.\n",
      "  psnr = skimage.measure.compare_psnr(rgb, gt, data_range=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object number 1\n",
      "object number 2\n",
      "object number 3\n",
      "object number 4\n",
      "object number 5\n",
      "object number 6\n",
      "object number 7\n",
      "object number 8\n",
      "object number 9\n",
      "object number 10\n",
      "object number 11\n",
      "object number 12\n",
      "object number 13\n",
      "object number 14\n",
      "object number 15\n",
      "object number 16\n",
      "object number 17\n",
      "object number 18\n",
      "object number 19\n",
      "object number 20\n",
      "object number 21\n",
      "object number 22\n",
      "object number 23\n",
      "object number 24\n",
      "object number 25\n",
      "object number 26\n",
      "object number 27\n",
      "object number 28\n",
      "object number 29\n",
      "object number 30\n",
      "object number 31\n",
      "object number 32\n",
      "object number 33\n",
      "object number 34\n",
      "object number 35\n",
      "object number 36\n",
      "object number 37\n",
      "object number 38\n",
      "object number 39\n",
      "object number 40\n",
      "object number 41\n",
      "object number 42\n",
      "object number 43\n",
      "object number 44\n",
      "object number 45\n",
      "object number 46\n",
      "object number 47\n",
      "object number 48\n",
      "object number 49\n",
      "object number 50\n",
      "object number 51\n",
      "object number 52\n",
      "object number 53\n",
      "object number 54\n",
      "object number 55\n",
      "object number 56\n",
      "object number 57\n",
      "object number 58\n",
      "object number 59\n",
      "object number 60\n",
      "object number 61\n",
      "object number 62\n",
      "object number 63\n",
      "object number 64\n",
      "object number 65\n",
      "object number 66\n",
      "object number 67\n",
      "object number 68\n",
      "object number 69\n",
      "object number 70\n",
      "object number 71\n",
      "object number 72\n",
      "object number 73\n",
      "object number 74\n",
      "object number 75\n",
      "object number 76\n",
      "object number 77\n",
      "object number 78\n",
      "object number 79\n",
      "object number 80\n",
      "object number 81\n",
      "object number 82\n",
      "object number 83\n",
      "object number 84\n",
      "object number 85\n",
      "object number 86\n",
      "object number 87\n",
      "object number 88\n",
      "object number 89\n",
      "object number 90\n",
      "object number 91\n",
      "object number 92\n",
      "object number 93\n",
      "object number 94\n",
      "object number 95\n",
      "object number 96\n",
      "object number 97\n",
      "object number 98\n",
      "object number 99\n",
      "object number 100\n",
      "object number 101\n",
      "object number 102\n",
      "object number 103\n",
      "object number 104\n",
      "object number 105\n",
      "object number 106\n",
      "object number 107\n",
      "object number 108\n",
      "object number 109\n",
      "object number 110\n",
      "object number 111\n",
      "object number 112\n",
      "object number 113\n",
      "object number 114\n",
      "object number 115\n",
      "object number 116\n",
      "object number 117\n",
      "object number 118\n",
      "object number 119\n",
      "object number 120\n",
      "object number 121\n",
      "object number 122\n",
      "object number 123\n",
      "object number 124\n",
      "object number 125\n",
      "object number 126\n",
      "object number 127\n",
      "object number 128\n",
      "object number 129\n",
      "object number 130\n",
      "object number 131\n",
      "object number 132\n",
      "object number 133\n",
      "object number 134\n",
      "object number 135\n",
      "object number 136\n",
      "object number 137\n",
      "object number 138\n",
      "object number 139\n",
      "object number 140\n",
      "object number 141\n",
      "object number 142\n",
      "object number 143\n",
      "object number 144\n",
      "object number 145\n",
      "object number 146\n",
      "object number 147\n",
      "object number 148\n",
      "object number 149\n",
      "object number 150\n",
      "object number 151\n",
      "object number 152\n",
      "object number 153\n",
      "object number 154\n",
      "object number 155\n",
      "object number 156\n",
      "object number 157\n",
      "object number 158\n",
      "object number 159\n",
      "object number 160\n",
      "object number 161\n",
      "object number 162\n",
      "object number 163\n",
      "object number 164\n",
      "object number 165\n",
      "object number 166\n",
      "object number 167\n",
      "object number 168\n",
      "object number 169\n",
      "object number 170\n",
      "object number 171\n",
      "object number 172\n",
      "object number 173\n",
      "object number 174\n",
      "object number 175\n",
      "object number 176\n",
      "object number 177\n",
      "object number 178\n",
      "object number 179\n",
      "object number 180\n",
      "object number 181\n",
      "object number 182\n",
      "object number 183\n",
      "object number 184\n",
      "object number 185\n",
      "object number 186\n",
      "object number 187\n",
      "object number 188\n",
      "object number 189\n",
      "object number 190\n",
      "object number 191\n",
      "object number 192\n",
      "object number 193\n",
      "object number 194\n",
      "object number 195\n",
      "object number 196\n",
      "object number 197\n",
      "object number 198\n",
      "object number 199\n",
      "object number 200\n",
      "object number 201\n",
      "object number 202\n",
      "object number 203\n",
      "object number 204\n",
      "object number 205\n",
      "object number 206\n",
      "object number 207\n",
      "object number 208\n",
      "object number 209\n",
      "object number 210\n",
      "object number 211\n",
      "object number 212\n",
      "object number 213\n",
      "object number 214\n",
      "object number 215\n",
      "object number 216\n",
      "object number 217\n",
      "object number 218\n",
      "object number 219\n",
      "object number 220\n",
      "object number 221\n",
      "object number 222\n",
      "object number 223\n",
      "object number 224\n",
      "object number 225\n",
      "object number 226\n",
      "object number 227\n",
      "object number 228\n",
      "object number 229\n",
      "object number 230\n",
      "object number 231\n",
      "object number 232\n",
      "object number 233\n",
      "object number 234\n",
      "object number 235\n",
      "object number 236\n",
      "object number 237\n",
      "object number 238\n",
      "object number 239\n",
      "object number 240\n",
      "object number 241\n",
      "object number 242\n",
      "object number 243\n",
      "object number 244\n",
      "object number 245\n",
      "object number 246\n",
      "object number 247\n",
      "object number 248\n",
      "object number 249\n",
      "object number 250\n",
      "object number 251\n",
      "object number 252\n",
      "object number 253\n",
      "object number 254\n",
      "object number 255\n",
      "object number 256\n",
      "object number 257\n",
      "object number 258\n",
      "object number 259\n",
      "object number 260\n",
      "object number 261\n",
      "object number 262\n",
      "object number 263\n",
      "object number 264\n",
      "object number 265\n",
      "object number 266\n",
      "object number 267\n",
      "object number 268\n",
      "object number 269\n",
      "object number 270\n",
      "object number 271\n",
      "object number 272\n",
      "object number 273\n",
      "object number 274\n",
      "object number 275\n",
      "object number 276\n",
      "object number 277\n",
      "object number 278\n",
      "object number 279\n",
      "object number 280\n",
      "object number 281\n",
      "object number 282\n",
      "object number 283\n",
      "object number 284\n",
      "object number 285\n",
      "object number 286\n",
      "object number 287\n",
      "object number 288\n",
      "object number 289\n",
      "object number 290\n",
      "object number 291\n",
      "object number 292\n",
      "object number 293\n",
      "object number 294\n",
      "object number 295\n",
      "object number 296\n",
      "object number 297\n",
      "object number 298\n",
      "object number 299\n",
      "object number 300\n",
      "object number 301\n",
      "object number 302\n",
      "object number 303\n",
      "object number 304\n",
      "object number 305\n",
      "object number 306\n",
      "object number 307\n",
      "object number 308\n",
      "object number 309\n",
      "object number 310\n",
      "object number 311\n",
      "object number 312\n",
      "object number 313\n",
      "object number 314\n",
      "object number 315\n",
      "object number 316\n",
      "object number 317\n",
      "object number 318\n",
      "object number 319\n",
      "object number 320\n",
      "object number 321\n",
      "object number 322\n",
      "object number 323\n",
      "object number 324\n",
      "object number 325\n",
      "object number 326\n",
      "object number 327\n",
      "object number 328\n",
      "object number 329\n",
      "object number 330\n",
      "object number 331\n",
      "object number 332\n",
      "object number 333\n",
      "object number 334\n",
      "object number 335\n",
      "object number 336\n",
      "object number 337\n",
      "object number 338\n",
      "object number 339\n",
      "object number 340\n",
      "object number 341\n",
      "object number 342\n",
      "object number 343\n",
      "object number 344\n",
      "object number 345\n",
      "object number 346\n",
      "object number 347\n",
      "object number 348\n",
      "object number 349\n",
      "object number 350\n",
      "object number 351\n",
      "23.36412329994068\n",
      "0.8651115592072237\n"
     ]
    }
   ],
   "source": [
    "# Raymarcher\n",
    "\n",
    "max_num_instances = 2150\n",
    "samples_per_instance = 8\n",
    "\n",
    "# Import model\n",
    "from pyhocon import ConfigFactory\n",
    "conf = ConfigFactory.parse_file(f\"{root_dir}adaptive-volume-rendering/conf/default_mv.conf\")\n",
    "\n",
    "net = make_new_model(conf[\"model\"]).to(device=device)\n",
    "net.stop_encoder_grad = True\n",
    "\n",
    "# renderer = VolumeRenderer.from_conf(conf[\"normal_renderer\"]).to(device=device)\n",
    "# renderer = Raymarcher.from_conf(conf[\"raymarcher\"]).to(device=device)\n",
    "renderer = AdaptiveVolumeRenderer.from_conf(conf[\"adaptive_renderer\"]).to(device=device)\n",
    "rf_and_renderer = RadFieldAndRenderer(net, renderer).to(device=device)\n",
    "\n",
    "# Load pretrianed weights\n",
    "# model_path = f\"{root_dir}checkpoints/experiment/vr_{max_num_instances}cars_{samples_per_instance}samples_40000steps\"\n",
    "# model_path = f\"{root_dir}checkpoints/experiment/raymarcher_{max_num_instances}cars_{samples_per_instance}samples_400ksteps\"\n",
    "model_path = f\"{root_dir}checkpoints/experiment/avr_{max_num_instances}cars_{samples_per_instance}samples_40ksteps\"\n",
    "rf_and_renderer.load_weights(model_path)\n",
    "\n",
    "# metrics\n",
    "sl = 32\n",
    "val_dir = f\"{root_dir}data/cars_val\"\n",
    "n_val = 352\n",
    "psnr, ssim = get_psnr(net, rf_and_renderer, val_dir, n_val, sl, [64])\n",
    "print(psnr)\n",
    "print(ssim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (v3.9.7:1016ef3790, Aug 30 2021, 16:39:15) \n[Clang 6.0 (clang-600.0.57)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
